# Copyright 2022 The Brax Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Proximal policy optimization training.

See: https://arxiv.org/pdf/1707.06347.pdf
"""

from typing import Tuple

from brax.envs import wrappers
from brax.training import types
from brax.training import acting
from brax.training.types import Params, Transition
from brax.training.acme import running_statistics
from brax import envs
import flax
import jax
import jax.numpy as jnp

import predictor
from ppo import networks as ppo_networks

import collections
import functools

from predictor import TransformerLM, TrainingState
import moviepy

PID_state = collections.namedtuple(
    "PID_state", ["MI_prev", "I", "k_p", "k_i", "k_d", "MI_eps", "lm"]
)


@flax.struct.dataclass
class PPONetworkParams:
    """Contains training state for the learner."""

    policy: Params
    value: Params


@flax.struct.dataclass
class UPredictorNetworkParams:
    """Contains training state for the learner."""

    # policy: Params

    # train_model: TransformerLM(TrainConfig)
    transformer_params: TrainingState


@flax.struct.dataclass
class TrainingParams:
    """Contains training state for the learner."""

    ppo_params: PPONetworkParams
    predictor_params: UPredictorNetworkParams


@flax.struct.dataclass
class NormalizerParams:
    """Contains training state for the learner."""

    ppo_norm_params: running_statistics.RunningStatisticsState
    predictor_norm_params: running_statistics.RunningStatisticsState


def update_PID(MI, curr_PID_state):
    """PID Step"""
    # use a PID controller
    error = MI - curr_PID_state.MI_eps
    partial_error = MI - curr_PID_state.MI_prev
    partial = jnp.clip(partial_error, a_min=0, a_max=None)
    I = jnp.clip(curr_PID_state.I + error, a_min=0, a_max=None)
    new_lambda = jnp.clip(
        curr_PID_state.k_p * error + curr_PID_state.k_i * I + curr_PID_state.k_d * partial,
        a_max=None,
        a_min=0,
    )
    new_PID_state = PID_state(
        MI_prev=MI,
        I=I,
        k_p=curr_PID_state.k_p,
        k_i=curr_PID_state.k_i,
        k_d=curr_PID_state.k_d,
        MI_eps=curr_PID_state.MI_eps,
        lm=new_lambda,
    )
    return new_PID_state, (
        curr_PID_state.k_p * error,
        curr_PID_state.k_i * I,
        curr_PID_state.k_d * partial,
    )


def compute_gae(
    truncation: jnp.ndarray,
    termination: jnp.ndarray,
    rewards: jnp.ndarray,
    values: jnp.ndarray,
    bootstrap_value: jnp.ndarray,
    lambda_: float = 1.0,
    discount: float = 0.99,
):
    """Calculates the Generalized Advantage Estimation (GAE).

    Args:
      truncation: A float32 tensor of shape [T, B] with truncation signal.
      termination: A float32 tensor of shape [T, B] with termination signal.
      rewards: A float32 tensor of shape [T, B] containing rewards generated by
        following the behaviour policy.
      values: A float32 tensor of shape [T, B] with the value function estimates
        wrt. the target policy.
      bootstrap_value: A float32 of shape [B] with the value function estimate at
        time T.
      lambda_: Mix between 1-step (lambda_=0) and n-step (lambda_=1). Defaults to
        lambda_=1.
      discount: TD discount.

    Returns:
      A float32 tensor of shape [T, B]. Can be used as target to
        train a baseline (V(x_t) - vs_t)^2.
      A float32 tensor of shape [T, B] of advantages.
    """

    truncation_mask = 1 - truncation
    # Append bootstrapped value to get [v1, ..., v_t+1]
    values_t_plus_1 = jnp.concatenate([values[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
    deltas = rewards + discount * (1 - termination) * values_t_plus_1 - values
    deltas *= truncation_mask

    acc = jnp.zeros_like(bootstrap_value)
    vs_minus_v_xs = []

    def compute_vs_minus_v_xs(carry, target_t):
        lambda_, acc = carry
        truncation_mask, delta, termination = target_t
        acc = delta + discount * (1 - termination) * truncation_mask * lambda_ * acc
        return (lambda_, acc), (acc)

    (_, _), (vs_minus_v_xs) = jax.lax.scan(
        compute_vs_minus_v_xs,
        (lambda_, acc),
        (truncation_mask, deltas, termination),
        length=int(truncation_mask.shape[0]),
        reverse=True,
    )
    # Add V(x_s) to get v_s.
    vs = jnp.add(vs_minus_v_xs, values)

    vs_t_plus_1 = jnp.concatenate([vs[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
    advantages = (rewards + discount * (1 - termination) * vs_t_plus_1 - values) * truncation_mask
    return jax.lax.stop_gradient(vs), jax.lax.stop_gradient(advantages)


def get_MIs(
    data,
    timesteps,
    get_us_fn,
    params_both: TrainingParams,
    rng_key,
):
    """Calculate Mutual Information from predictor between us (hidden) and actions"""
    transformer_params = params_both.predictor_params.transformer_params
    us = get_us_fn(data.observation)
    us = jnp.transpose(us, (1, 0))  # B x H
    actions = data.extras["policy_extras"]["raw_action"]  # H X B x 7
    actions = jnp.transpose(actions, (1, 0, 2))  # B x H x 7

    data_batch = predictor.Batch(inputs=actions, targets=us)

    # TODO: confirm that is_training is FALSE!!!!
    predictor_logits = predictor.get_logits(
        transformer_params, rng_key, data_batch, pos_ids=timesteps, is_training=False
    )

    targets = jax.nn.one_hot(us, num_classes=2)
    target_hidden_log_probs = jnp.sum(targets * jax.nn.log_softmax(predictor_logits), axis=-1)
    return jnp.mean(target_hidden_log_probs + jnp.log(2))


def mle_loss_fn_from_both(
    joint_params,
    rng,
    data,
    timesteps,
    horizon,
    subsample_batch_size,
    get_us_fn,
    is_training=True,
) -> jnp.ndarray:
    return mle_loss_fn(
        joint_params.predictor_params.transformer_params,
        rng,
        data,
        timesteps,
        horizon,
        subsample_batch_size,
        get_us_fn,
        is_training,
    )


def mle_loss_fn(
    transformer_params,
    rng,
    data,
    timesteps,
    horizon,
    subsample_batch_size,
    get_us_fn,
    is_training=True,
) -> jnp.ndarray:
    """Computes the (scalar) LM loss on `data` w.r.t. params.
    Actions: Bxunroll_lengthxA, where A=action size,
    us: B,"""
    # The architecture gives out *NON-SHIFTED* logits.
    # So given an input [a, b, c], the outputs are [logits(target | a), logits(target | a,b), logits(target | a,b,c)]

    us = get_us_fn(data.observation)
    actions = data.extras["policy_extras"]["raw_action"]
    B, unroll_length, _ = actions.shape
    if unroll_length <= horizon:
        raise ValueError("unroll_length must be greater than horizon")

    subsample_indices = jax.random.choice(
        rng,
        unroll_length - horizon,
        shape=(B, subsample_batch_size),
        replace=True,
    )

    all_indices = jnp.concatenate(
        [subsample_indices[..., None], jnp.ones((B, subsample_batch_size, horizon - 1))], axis=-1
    )
    all_indices = jnp.cumsum(all_indices, axis=-1)

    actions_inputs = jnp.take_along_axis(
        actions, all_indices.reshape(B, -1, 1).astype(jnp.int32), axis=1
    ).reshape(B * subsample_batch_size, horizon, -1)

    # Positional Embeddings, which we don't use
    # pos_idx_inputs = all_indices.reshape(B * subsample_batch_size, -1)

    timesteps_inputs = jnp.take_along_axis(
        timesteps, all_indices.reshape(B, -1).astype(jnp.int32), axis=1
    ).reshape(B * subsample_batch_size, horizon)

    extended_us = jnp.tile(us[:, 0].reshape(B, 1, 1), (1, subsample_batch_size, horizon)).reshape(
        B * subsample_batch_size, horizon
    )

    data_batch = predictor.Batch(inputs=actions_inputs, targets=extended_us)
    predictor_logits = predictor.get_logits(
        transformer_params, rng, data_batch, is_training, pos_ids=timesteps_inputs.astype(jnp.int32)
    )

    targets = jax.nn.one_hot(extended_us, num_classes=2)
    log_likelihood = jnp.sum(targets * jax.nn.log_softmax(predictor_logits), axis=-1)
    return -jnp.mean(log_likelihood)


def compute_ppo_loss(
    params_both: TrainingParams,
    normalizer_params: NormalizerParams,
    data: types.Transition,
    states: envs.State,
    rng: jnp.ndarray,
    curr_PID_state: PID_state,
    ppo_network: ppo_networks.PPONetworks,
    entropy_cost: float = 1e-4,
    discounting: float = 0.9,
    reward_scaling: float = 1.0,
    lambda_: float = 0.95,
    ppo_epsilon: float = 0.3,
    get_us_fn=lambda x: x,
    env=None,
    horizon=10,
    n_truncated_rollouts=128,
    subsample_batch_size=5,
    T=100,
    action_repeat=1,
    do_regularization=True,
) -> Tuple[jnp.ndarray, types.Metrics]:
    """Computes PPO loss.

    Args:
      params_both: Network parameters,
      normalizer_params: Parameters of the normalizer.
      data: Transition that with leading dimension [B, T]. extra fields required
        are: ['state_extras']['truncation'] ['policy_extras']['raw_action']
          ['policy_extras']['log_prob']
      states: the states
      rng: Random key
      curr_PID_state: PID state
      ppo_network: PPO networks.
      entropy_cost: entropy cost.
      discounting: discounting,
      reward_scaling: reward multiplier.
      lambda_: General advantage estimation lambda.
      ppo_epsilon: PPO epsilon
      get_us_fn: env's hidden variables,
      env: environment,
      horizon: truncated trajectory length,
      n_truncated_rollouts: number of truncated rollouts,
      subsample_batch_size: batch size for training predictor (transformer)
      T: episode_length
    Returns:
      A tuple (loss, metrics)
    """
    ppo_params = params_both.ppo_params
    predictor_params = params_both.predictor_params

    ppo_norm_params = normalizer_params.ppo_norm_params

    parametric_action_distribution = ppo_network.parametric_action_distribution
    policy_apply = ppo_network.policy_network.apply
    value_apply = ppo_network.value_network.apply

    transformer_params = predictor_params.transformer_params

    # TODO: confirm that is_training is FALSE!!!!
    predictor_loss = mle_loss_fn(
        transformer_params,
        rng,
        data,
        states.info["steps"],
        horizon,
        subsample_batch_size,
        get_us_fn,
        False,
    )

    policy = ppo_networks.make_inference_fn(ppo_network)(
        (
            ppo_norm_params,
            params_both.ppo_params.policy,
        )
    )

    def unroll_truncated_trajectory(init_state, rng_key):
        """Truncated trajectories of length <horizon>"""
        return acting.generate_unroll(
            env,
            init_state,
            policy,
            rng_key,
            horizon,
        )

    # TODO: rename first_qps/ first_obs / first_reward variables
    first_qps = states.qp
    first_obs = states.obs

    rng, rng_1 = jax.random.split(rng, 2)

    subsample_idxs = jax.random.choice(
        rng,
        int(first_qps.pos.shape[0] * first_qps.pos.shape[1]),
        shape=(n_truncated_rollouts,),
        replace=False,
    )
    first_obs, first_qps, first_rewards, first_steps, first_truncation = jax.tree_map(
        lambda x: x.reshape(-1, *x.shape[2:])[subsample_idxs],
        (first_obs, first_qps, data.reward, states.info["steps"], states.info["truncation"]),
    )
    first_dones = jnp.zeros(n_truncated_rollouts)
    inputs = envs.State(
        obs=first_obs,
        qp=first_qps,
        reward=first_rewards,
        done=first_dones,
        metrics={
            # These metrics are the combined metrics of Pusher and Ant
            "reward_dist": first_rewards,
            "reward_near": first_rewards,
            "reward_forward": first_rewards,
            "reward_survive": first_rewards,
            "reward_ctrl": first_rewards,
            "reward_contact": first_rewards,
            "x_position": first_rewards,
            "y_position": first_rewards,
            "distance_from_origin": first_rewards,
            "x_velocity": first_rewards,
            "y_velocity": first_rewards,
            "forward_reward": first_rewards,
            # Only t matters here
            "t": first_steps,
        },
        info={
            "first_qp": first_qps,
            "first_obs": first_obs,
            "steps": first_steps,
            "truncation": first_truncation,
        },
    )
    _, trajectories = unroll_truncated_trajectory(inputs, rng_1)

    timesteps = jnp.concatenate(
        [first_steps[..., None], jnp.ones((first_steps.shape[0], horizon - 1))], axis=-1
    ).astype(jnp.int32)
    timesteps = jnp.cumsum(timesteps, axis=-1)
    timesteps = jnp.where(timesteps > T - 1, timesteps - T, timesteps)

    truncated_MIs = get_MIs(
        trajectories,
        timesteps,
        get_us_fn,
        TrainingParams(  # type: ignore
            ppo_params=params_both.ppo_params,
            predictor_params=jax.lax.stop_gradient(params_both.predictor_params),
        ),
        None,
    )

    new_PID_state, (p_lm_term, i_lm_term, d_lm_term) = update_PID(truncated_MIs, curr_PID_state)

    # Put the time dimension first.
    data = jax.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)
    policy_logits = policy_apply(ppo_norm_params, ppo_params.policy, data.observation)

    baseline = value_apply(ppo_norm_params, ppo_params.value, data.observation)

    bootstrap_value = value_apply(ppo_norm_params, ppo_params.value, data.next_observation[-1])

    rewards = data.reward * reward_scaling
    truncation = data.extras["state_extras"]["truncation"]
    termination = (1 - data.discount) * (1 - truncation)

    target_action_log_probs = parametric_action_distribution.log_prob(
        policy_logits, data.extras["policy_extras"]["raw_action"]
    )
    behaviour_action_log_probs = data.extras["policy_extras"]["log_prob"]

    vs, advantages = compute_gae(
        truncation=truncation,
        termination=termination,
        rewards=rewards,
        values=baseline,
        bootstrap_value=bootstrap_value,
        lambda_=lambda_,
        discount=discounting,
    )
    rho_s = jnp.exp(target_action_log_probs - behaviour_action_log_probs)

    surrogate_loss1 = rho_s * advantages
    surrogate_loss2 = jnp.clip(rho_s, 1 - ppo_epsilon, 1 + ppo_epsilon) * advantages

    policy_loss = -jnp.mean(jnp.minimum(surrogate_loss1, surrogate_loss2))

    # Value function loss
    v_error = vs - baseline
    v_loss = jnp.mean(v_error * v_error) * 0.5 * 0.5

    # Entropy reward
    entropy = jnp.mean(parametric_action_distribution.entropy(policy_logits, rng))
    entropy_loss = entropy_cost * -entropy
    MI_loss = truncated_MIs * curr_PID_state.lm

    total_loss = policy_loss + v_loss + entropy_loss + predictor_loss + MI_loss
    return total_loss, {
        "total_loss": total_loss,
        "policy_loss": policy_loss,
        "v_loss": v_loss,
        "entropy_loss": entropy_loss,
        "predictor_loss": predictor_loss,
        "PID_state": new_PID_state,
        "lm": new_PID_state.lm,
        "truncated_MI": truncated_MIs,
        "effective_truncated_loss": MI_loss,
        "p_lm_contribution": p_lm_term,
        "i_lm_contribution": i_lm_term,
        "d_lm_contribution": d_lm_term,
    }


def compute_ppo_loss_unconstrained(
    params_both: TrainingParams,
    normalizer_params: NormalizerParams,
    data: types.Transition,
    states: envs.State,
    rng: jnp.ndarray,
    curr_PID_state: PID_state,
    ppo_network: ppo_networks.PPONetworks,
    entropy_cost: float = 1e-4,
    discounting: float = 0.9,
    reward_scaling: float = 1.0,
    lambda_: float = 0.95,
    ppo_epsilon: float = 0.3,
    get_us_fn=lambda x: x,
    env=None,
    horizon=10,
    n_truncated_rollouts=128,
    subsample_batch_size=5,
    T=100,
    action_repeat=1,
    do_regularization=True,
) -> Tuple[jnp.ndarray, types.Metrics]:
    """Computes PPO loss without regularization."""

    ppo_params = params_both.ppo_params
    predictor_params = params_both.predictor_params

    ppo_norm_params = normalizer_params.ppo_norm_params

    parametric_action_distribution = ppo_network.parametric_action_distribution
    policy_apply = ppo_network.policy_network.apply
    value_apply = ppo_network.value_network.apply

    transformer_params = predictor_params.transformer_params

    policy = ppo_networks.make_inference_fn(ppo_network)(
        (
            ppo_norm_params,
            params_both.ppo_params.policy,
        )
    )

    rng, rng_1 = jax.random.split(rng, 2)

    # Put the time dimension first.
    data = jax.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)
    policy_logits = policy_apply(ppo_norm_params, ppo_params.policy, data.observation)

    baseline = value_apply(ppo_norm_params, ppo_params.value, data.observation)

    bootstrap_value = value_apply(ppo_norm_params, ppo_params.value, data.next_observation[-1])

    rewards = data.reward * reward_scaling
    truncation = data.extras["state_extras"]["truncation"]
    termination = (1 - data.discount) * (1 - truncation)

    target_action_log_probs = parametric_action_distribution.log_prob(
        policy_logits, data.extras["policy_extras"]["raw_action"]
    )
    behaviour_action_log_probs = data.extras["policy_extras"]["log_prob"]

    vs, advantages = compute_gae(
        truncation=truncation,
        termination=termination,
        rewards=rewards,
        values=baseline,
        bootstrap_value=bootstrap_value,
        lambda_=lambda_,
        discount=discounting,
    )
    rho_s = jnp.exp(target_action_log_probs - behaviour_action_log_probs)

    surrogate_loss1 = rho_s * advantages
    surrogate_loss2 = jnp.clip(rho_s, 1 - ppo_epsilon, 1 + ppo_epsilon) * advantages

    policy_loss = -jnp.mean(jnp.minimum(surrogate_loss1, surrogate_loss2))

    # Value function loss
    v_error = vs - baseline
    v_loss = jnp.mean(v_error * v_error) * 0.5 * 0.5

    # Entropy reward
    entropy = jnp.mean(parametric_action_distribution.entropy(policy_logits, rng))
    entropy_loss = entropy_cost * -entropy

    total_loss = policy_loss + v_loss + entropy_loss
    return total_loss, {
        "total_loss": total_loss,
        "policy_loss": policy_loss,
        "v_loss": v_loss,
        "entropy_loss": entropy_loss,
        "predictor_loss": jnp.nan,
        "PID_state": curr_PID_state,
        "lm": jnp.nan,
        "truncated_MI": jnp.nan,
        "effective_truncated_loss": jnp.nan,
        "p_lm_contribution": jnp.nan,
        "i_lm_contribution": jnp.nan,
        "d_lm_contribution": jnp.nan,
    }
